{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "‚û°Ô∏è Before you start, make sure that you are familiar with the **[study guide](https://liu-nlp.ai/text-mining/logistics/)**, in particular the rules around **cheating and plagiarism** (found in the course memo).\n",
    "\n",
    "‚û°Ô∏è If you use code from external sources (e.g. StackOverflow, ChatGPT, ...) as part of your solutions, don't forget to add a reference to these source(s) (for example as a comment above your code).\n",
    "\n",
    "‚û°Ô∏è Make sure you fill in all cells that say **`YOUR CODE HERE`** or **YOUR ANSWER HERE**.  You normally shouldn't need to modify any of the other cells.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# L2: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification is the task of sorting text documents into predefined classes. The concrete problem you will be working on in this lab is the classification of speeches with respect to their political affiliation. The specific texts you are going to classify are speeches held in the [Riksdag](https://www.riksdagen.se/en/), the Swedish national legislature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data for this lab comes from [The Riksdag‚Äôs Open Data](https://data.riksdagen.se/in-english/). We have preprocessed and tokenized the speeches and put them into two compressed [JSON](https://en.wikipedia.org/wiki/JSON) files:\n",
    "\n",
    "* `speeches-201718.json.bz2` (speeches from the 2017/2018 parliamentary session)\n",
    "* `speeches-201819.json.bz2` (ditto, from the 2018/2019 session)\n",
    "\n",
    "We start by loading these files into two separate data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbd62577bd7843909416d134d72e1194",
     "grade": false,
     "grade_id": "cell-381ec95ffbde9678",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bz2\n",
    "\n",
    "with bz2.open('speeches-201718.json.bz2', mode='rt', encoding='utf-8') as source:\n",
    "    speeches_201718 = pd.read_json(source, encoding='utf-8')\n",
    "\n",
    "with bz2.open('speeches-201819.json.bz2', mode='rt', encoding='utf-8') as source:\n",
    "    speeches_201819 = pd.read_json(source, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you inspect the two data frames, you can see that there are three labelled columns: `id` (the official speech ID), `words` (the space-separated words of the speech), and `party` (the party of the speaker, represented by its customary abbreviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_201718.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the lab, we will be using the speeches from **2017/2018** as our **training data**, and the speeches from **2018/2019** as our **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e79a1e65d8e7b70c0662a5454e1e68a5",
     "grade": false,
     "grade_id": "cell-d4057991468fe3ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "training_data, test_data = speeches_201718, speeches_201819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later reference, we store the sorted list of party abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = sorted(training_data['party'].unique())\n",
    "print(parties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs start by getting to know the data better by producing a simple visualization.  More specifically, we ask you to compare the two data frames with respect to the distribution of the speeches over the different parties.\n",
    "\n",
    "_(If you are not familiar with the Swedish political system and the parties represented in the Riksdag in particular, then we suggest that you have a look at the Wikipedia article about the [2018 Swedish general election](https://en.wikipedia.org/wiki/2018_Swedish_general_election), but it‚Äôs not required for solving this lab.  Neither is knowing Swedish!)_\n",
    "\n",
    "Your task is to **write code to generate two bar plots** that visualize the party distribution, one for the 2017/2018 speeches and one for the 2018/2019 speeches. Inspect the two plots, and compare them\n",
    "\n",
    "* to each other; and\n",
    "* to the results of the 2014 and the 2018 general elections.\n",
    "\n",
    "To facilitate the comparison, you should plot the parties in descending order of their total number of speeches.\n",
    "\n",
    "**_Note:_** You can produce the bar plots in any way you like (e.g. with Matplotlib, Seaborn, or directly with Pandas); if you don‚Äôt know where to start, one suggestion is to look at [seaborn‚Äôs tutorial for ‚Äúvisualizing categorical data‚Äù](https://seaborn.pydata.org/tutorial/categorical.html), specifically the parts about making _count_ plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing seaborn style ‚Äî usually a good choice by default\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b52acc4f999f4811ad112c473dc61b07",
     "grade": true,
     "grade_id": "cell-435dbe643962a0e1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"Produce a plot for the 2017/2018 speeches.\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70818efe11a780503408efbca9e43c89",
     "grade": true,
     "grade_id": "cell-8b8855796909494a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"Produce a plot for the 2018/2019 speeches.\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "After you made the plots, **summarize your observations** very briefly in the cell below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6e2646f15ce9856a929ffb804c604e9",
     "grade": true,
     "grade_id": "cell-f17b6c18a5f3684c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to train and evaluate a classifier. More specifically, we ask you to train a [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) classifier. You will have to\n",
    "\n",
    "1. vectorize the speeches in the training data _(with simple token counts)_\n",
    "2. instantiate and fit the Naive Bayes model on the training data\n",
    "3. evaluate the model on the test data\n",
    "\n",
    "You can use any approach you like, but a suggestion is to use scikit-learn‚Äôs convenience [Pipeline class](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that allows you to solve the first two tasks with very compact code. For the evaluation you can use the function [classification_report()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), which will report per-class precision, recall and F1, as well as overall accuracy.\n",
    "\n",
    "**Write code to produce a Multinomial Naive Bayes classifier as described above and report its performance on the test data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76cea169d2a834695abe2643fce1f538",
     "grade": true,
     "grade_id": "cell-b311b60eb7e5a413",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would you have expected the results that you got? (No need to write anything about this just yet, but make sure you understand what the values in the report mean.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics such as accuracy or F1-score should not be understood as _absolute_ measures of performance, but should be used only to _compare_ different classifiers. When other classifiers are not available, there are some simple baselines we can use that are implemented by scikit-learn‚Äôs [DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html).\n",
    "\n",
    "One of the simplest baseline is to predict, for every document, that class which appears most often in the training data. This baseline is also called the _most frequent class_ baseline.\n",
    "\n",
    "**Write code to fit the most-frequent-class baseline on the training data and report its performance on the test data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e61c3fc37d9f653081e9b0fcff3e4106",
     "grade": true,
     "grade_id": "cell-d487ca84ccab29cd",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slightly more interesting baseline is a classifier that generates predictions by random sampling, while respecting the training set‚Äôs class distribution.  This way of random sampling is sometimes called _stratified sampling_.\n",
    "\n",
    "**Write code to fit the random sampling baseline on the training data and report its performance on the test data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af93eece5180010d1c22e6a18fd27c58",
     "grade": true,
     "grade_id": "cell-b24fc752be5cfcea",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Creating a balanced data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you saw in Problem&nbsp;1, the distribution of the speeches over the eight different parties (classes) is imbalanced. One technique used to alleviate this is **undersampling**, in which one randomly removes samples from over-represented classes until all classes are represented with the same number of samples.\n",
    "\n",
    "Your task here is to **implement undersampling to create a balanced subset of the training data**. Afterwards, **rerun the evaluation** from Problem&nbsp;2 on the balanced data and compare the results.\n",
    "\n",
    "**_Hint:_** Your balanced subset should consist of 5,752 speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12b02ac4a208566cfcba76b45948f97d",
     "grade": true,
     "grade_id": "cell-745500aace13dd25",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"Implement undersampling with the classifier from Problem 2 and report its performance on the test data.\"\"\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _confusion matrix_ is a type of table that is useful when analysing the performance of a classifier. In this table, both the rows and the columns correspond to classes, and each cell $(i, j)$ states how many times a sample with _gold-standard_ class $i$ was _predicted_ as belonging to class $j$.\n",
    "\n",
    "In scitkit-learn, a simple way to visualize the confusion matrix as a _heatmap_ is to use [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html).  The following cell demonstrates how to compute and display a confusion matrix for the classifier in the `model` variable ‚Äî you may have to adjust this based on the code you have written for your previous solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "with sns.axes_style(\"white\"):  # Seaborn‚Äôs default style doesn‚Äôt play well with ConfusionMatrixDisplay, so we change it temporarily\n",
    "    ConfusionMatrixDisplay.from_estimator(\n",
    "        model,                 # The model that you want to plot the confusion matrix for\n",
    "        test_data['words'],    # The input data for the model\n",
    "        test_data['party'],    # The correct (gold-standard) labels for the input data\n",
    "        normalize='true',\n",
    "        values_format='.2f'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task** is to use the confusion matrix in order to find, for each given party $p$ in the Riksdag, that other party $p'$ which the classifier that you trained in Problem&nbsp;4 most often confuses with $p$ when it predicts the party of a speaker.  You can read this off of the visualization above, but for the purposes of this exercise, we want to do this programmatically (i.e. with code).  For this, you most likely want to use the scikit-learn function [confusion_matrix()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), which gives you the confusion matrix of a classifier.\n",
    "\n",
    "**Write code that, for each party $p$, print the party most often confused with $p$ by the classifier from Problem 4!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cd82f4488fd2d66933c6bbdfe1179e3",
     "grade": true,
     "grade_id": "cell-200a41c181d159a3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you move on, take a minute to reflect on whether your results make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, you have been using the count-based vectorizer and the Naive Bayes classifier with their default hyperparameters. When working with real-world applications, you would want to find a combination of input representation, model, and hyperparameters that maximizes the performance for the task at hand.\n",
    "\n",
    "Manually trying out many combinations of hyperparameters of a vectorizer‚Äìclassifier pipeline can be cumbersome. However, scikit-learn provides functionality for [performing **grid search**](https://scikit-learn.org/stable/modules/grid_search.html), which is the name for an exhaustive search for the best hyperparameters over a _grid_ of possible values.\n",
    "\n",
    "When searching for the best hyperparameters, it‚Äôs important to _not_ run evaluations on the final test set.  Instead, one should either use a separate validation set, or run cross-validation over different folds. Here we will use cross-validation, which scikit-learn provides in the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class.  To use grid search, you need to know which parameters can be tuned and what they‚Äôre called; you can find this information by calling `get_params()` on your model pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"nb_pipe\" with the variable that has your model pipeline, if necessary\n",
    "nb_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to **find a pipeline that outperforms the model from Problem&nbsp;4** on _at least one_ of the aggregated evaluation metrics (accuracy, macro avg., or weighted avg. scores) on the test set, by performing **grid search with 5-fold cross-validation** on the training set.  Which hyperparameters you tune and what values you choose is up to you, as long as try at least _two different hyperparameters_ with at least _two different values_ each, and your resulting pipelines outperforms the one from Problem&nbsp;4.  However, here are some suggestions:\n",
    "\n",
    "* In the vectorizer, you could try a _set-of-words_ (binary) model in addition to the default _bag-of-words_ model.\n",
    "* In the vectorizer, you could try extracting bigrams and/or trigrams in addition to unigrams.\n",
    "* In the Naive Bayes classifier, you could try to using different values for additive smoothing.\n",
    "\n",
    "**Write code to perform grid search, then report the results of your best model, along with the parameter values that yielded these results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fec6c6f6c8f459b01e145223c705f3a",
     "grade": true,
     "grade_id": "cell-fc4a0a81ba69cbf7",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual reflection\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <strong>After you have solved the lab,</strong> write a <em>brief</em> reflection (max. one A4 page) on the question(s) below.  Remember:\n",
    "    <ul>\n",
    "        <li>You are encouraged to discuss this part with your lab partner, but you should each write up your reflection <strong>individually</strong>.</li>\n",
    "        <li><strong>Do not put your answers in the notebook</strong>; upload them in the separate submission opportunity for the reflections on Lisam.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you have produced evaluation reports for a total of five classifiers:\n",
    "\n",
    "- two baselines (most frequent class & random sampling)\n",
    "- Naive Bayes with default values and unbalanced data\n",
    "- Naive Bayes with default values and undersampled data\n",
    "- the best-performing pipeline from your grid search\n",
    "\n",
    "How do you interpret these evaluation reports, i.e., what do they mean for the usefulness of each classifier? What conclusion do you draw from these results? Refer to specific evaluation scores that you find particularly relevant for your argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ccdbd-4375-4d2f-8b1d-f47097ef2e84",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this lab! üëç**\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "‚û°Ô∏è Before you submit, **make sure the notebook can be run from start to finish** without errors.  For this, _restart the kernel_ and _run all cells_ from top to bottom. In Jupyter Notebook version 7 or higher, you can do this via \"Run$\\rightarrow$Restart Kernel and Run All Cells...\" in the menu (or the \"‚è©\" button in the toolbar).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad192d-7557-4cd9-9ead-6699b8de9114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
